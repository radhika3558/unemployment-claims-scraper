# Auto-updating US Jobless Claims Visualization README

## Introduction
üìä This repository hosts an innovative project that automatically scrapes, processes, and visualizes the US Department of Labor's weekly jobless claims data.

## Workflow Details

### Step 1: Data Scraping and Processing
- **Tool Utilization:** Utilizes BeautifulSoup for web scraping and Camelot for PDF data extraction.
- **Data Handling:** Extracts relevant data from the Department of Labor's PDFs, cleans, and converts it into a structured CSV format (`statewise-claims.csv`).

### Step 2: GitHub Integration
- **Repository Management:** Involves setting up a GitHub repository to store and manage the project's code and data.
- **Automation with GitHub Actions:** Implements GitHub Actions to automate the scraping process using a YAML (`.yml`) configuration file.

### Step 3: Webpage Development
- **HTML Page Creation:** Designs an HTML page (`index.html`) for data visualization.
- **Deployment via GitHub Pages:** Employs GitHub Pages for hosting the visualization webpage, enabling public accessibility.

### Step 4: Data Visualization and Embedding
- **Visualization Tool:** Creates an interactive chart using DataWrapper, providing an insightful and user-friendly representation of the data.
- **Embedding in Webpage:** Integrates the visualization into `index.html` using embed codes from DataWrapper.
- **Update and Deployment:** Regular updates to the visualization are pushed to GitHub Pages, ensuring the data displayed is current.

## Contribution and Collaboration
ü§ù Contributions are highly encouraged. Feel free to fork the repository, make improvements, and submit pull requests. Open issues for any bugs or feature suggestions.

